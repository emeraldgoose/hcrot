{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from hcrot import layers, optim\n",
    "from hcrot.dataset import *\n",
    "from hcrot.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/data.txt', 'r', encoding='utf-8') as f:\n",
    "    sentences = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "PUNCTUATIONS = ['?','!','@','#','$','%','&','*','(',')','-','=','+',',','.',';',':','\\'']\n",
    "LOWER_ALPHAS = [chr(i) for i in range(97,97+26)]\n",
    "UPPER_ALPHAS = [chr(i) for i in range(65,65+26)]\n",
    "NUMBERS = ['0','1','2','3','4','5','6','7','8','9']\n",
    "WHITE = chr(0x0120)\n",
    "SPECIAL_TOKENS = [\"<eos>\", \"<pad>\", \"<unk>\"]\n",
    "\n",
    "dictionary = defaultdict(int)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokenized = re.findall(r\"\\w+|[^\\w\\s]\", sentence)\n",
    "    for i, word in enumerate(tokenized):\n",
    "        word = WHITE + word if i != 0 else word\n",
    "        dictionary[word] += 1\n",
    "\n",
    "vocabulary =  PUNCTUATIONS + LOWER_ALPHAS + UPPER_ALPHAS + SPECIAL_TOKENS + [WHITE] + NUMBERS\n",
    "MAX_ITERATIONS = 10000\n",
    "max_vocab_size = 100000\n",
    "iterations = 0\n",
    "\n",
    "def split_word(word, vocab):\n",
    "    i, word_splited = 0, []\n",
    "    while i < len(word):\n",
    "        matched = False\n",
    "        for j in range(len(word), i, -1):\n",
    "            if word[i:j] in vocab:\n",
    "                word_splited.append(word[i:j])\n",
    "                i = j\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            word_splited.append(word[i])\n",
    "            i += 1\n",
    "    return word_splited\n",
    "\n",
    "while iterations < MAX_ITERATIONS and len(vocabulary) < max_vocab_size:\n",
    "    pairs = defaultdict(int)\n",
    "    for word, cnt in dictionary.items():\n",
    "        tokens = split_word(word, set(vocabulary))\n",
    "        for a, b in zip(tokens[:-1],tokens[1:]):\n",
    "            pairs[a + b] += cnt\n",
    "        \n",
    "    if not pairs:\n",
    "        break\n",
    "    \n",
    "    best_pair = max(pairs.items(), key=lambda x: x[1])[0]\n",
    "    vocabulary.append(best_pair)\n",
    "    \n",
    "    new_dictionary = defaultdict(int)\n",
    "    for word, cnt in dictionary.items():\n",
    "        tokens = split_word(word, set(vocabulary))\n",
    "        new_word = ''.join(tokens)\n",
    "        new_dictionary[new_word] += cnt\n",
    "    dictionary = new_dictionary\n",
    "    iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, vocab: list, max_len = 50):\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.token2ids = {token:i for i, token in enumerate(vocab)}\n",
    "        self.ids2token = {i:token for token, i in self.token2ids.items()}\n",
    "        \n",
    "        self.white = chr(0x0120)\n",
    "        self.EOS = self.token2ids[\"<eos>\"]\n",
    "        self.PAD = self.token2ids[\"<pad>\"]\n",
    "        self.UNK = self.token2ids[\"<unk>\"]\n",
    "        self.special_tokens = [self.EOS, self.PAD, self.UNK]\n",
    "\n",
    "    def __call__(self, inputs: list):\n",
    "        token_ids = self.encode(inputs)\n",
    "        for i, ids in enumerate(token_ids):\n",
    "            token_ids[i] = ids + [self.EOS] + [self.PAD] * (self.max_len - len(ids) - 1)\n",
    "        return np.array(token_ids)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def encode(self, inputs: list):\n",
    "        if isinstance(inputs, str):\n",
    "            inputs = [inputs]\n",
    "        \n",
    "        encoded = []\n",
    "        for string in inputs:\n",
    "            tokens = self.tokenize(string)\n",
    "            ids = [self.token2ids[token] for token in tokens]\n",
    "            encoded.append(ids)\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, token_ids: list, skip_special_tokens: bool = False):\n",
    "        decoded = []\n",
    "        for tokens in token_ids:\n",
    "            if not skip_special_tokens:\n",
    "                tokens = [self.ids2token[token] for token in tokens]\n",
    "            else:\n",
    "                tokens = [self.ids2token[token] for token in tokens if token not in self.special_tokens]\n",
    "            tokens = ''.join(tokens)\n",
    "            decoded.append(tokens.replace(self.white, ' '))\n",
    "        return decoded\n",
    "\n",
    "    def tokenize(self, sentence: str):\n",
    "        tokens = []\n",
    "        for i, word in enumerate(sentence.split()):\n",
    "            word = self.white + word if i != 0 else word\n",
    "            tokens += self.word_tokenize(word)\n",
    "        return tokens\n",
    "\n",
    "    def word_tokenize(self, word: str):\n",
    "        while True:\n",
    "            i, r = 0, []\n",
    "            while i < len(word):\n",
    "                if i + 1 < len(word) and (word[i] + word[i+1]) in self.vocab:\n",
    "                    r.append(word[i] + word[i+1])\n",
    "                    i += 1\n",
    "                elif word[i] in self.vocab:\n",
    "                    r.append(word[i])\n",
    "                else:\n",
    "                    r.append(self.UNK)\n",
    "                i += 1\n",
    "            if word == r:\n",
    "                break\n",
    "            word = r\n",
    "        return word\n",
    "    \n",
    "tokenizer = BPETokenizer(vocab=vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
    "    # refs: https://paul-hyun.github.io/transformer-01/\n",
    "    def cal_angle(position, i_hidn):\n",
    "        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\n",
    "\n",
    "    return sinusoid_table\n",
    "\n",
    "class GPT(layers.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, max_len=16):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding = layers.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = np.expand_dims(get_sinusoid_encoding_table(max_len, embed_size), axis=0)\n",
    "        self.transformer_decoder_layer = layers.TransformerDecoderLayer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_size * 4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = layers.TransformerDecoder(\n",
    "            self.transformer_decoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.fc_out = layers.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, tgt):\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_mask = self._generate_square_subsequent_mask(tgt_len)\n",
    "\n",
    "        tgt_emb = self.embedding(tgt) + self.positional_encoding[:, :tgt_len, :]\n",
    "\n",
    "        output = self.transformer_decoder(tgt_emb, tgt_emb, tgt_mask=tgt_mask)\n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = np.triu(np.ones((sz, sz)), 1)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.00001: 100%|██████████| 200/200 [09:30<00:00,  2.85s/it]\n"
     ]
    }
   ],
   "source": [
    "embed_size = 64\n",
    "num_heads = 4\n",
    "num_layers = 1\n",
    "max_len = 52\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "model = GPT(vocab_size, embed_size, num_heads, num_layers, max_len=max_len)\n",
    "# model.load_state_dict(load('notebooks/gpt.pickle'))\n",
    "\n",
    "criterion = layers.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model, lr_rate=1e-4, betas=(0.9, 0.95))\n",
    "\n",
    "tokenized = tokenizer(sentences)\n",
    "inputs = tokenized[:, :-1]\n",
    "targets = tokenized[:, 1:]\n",
    "\n",
    "dataloader = Dataloader(inputs, targets, batch_size=4, shuffle=True)\n",
    "mini_batch_len = len(dataloader)\n",
    "\n",
    "num_epochs = 600\n",
    "pbar = tqdm(range(num_epochs))\n",
    "for epoch in pbar:\n",
    "    total_loss = 0\n",
    "    for inputs_, targets_ in dataloader:\n",
    "        bsz, seq_len = inputs_.shape\n",
    "        outputs = model(inputs_)\n",
    "        outputs = outputs.reshape(-1, vocab_size)\n",
    "        targets_ = targets_.reshape(-1)\n",
    "        loss = criterion(outputs, targets_)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        dz = criterion.backward()\n",
    "        dz = dz.reshape(bsz, seq_len, -1)\n",
    "        optimizer.update(dz)\n",
    "    \n",
    "    pbar.set_description(f'Loss: {total_loss/mini_batch_len:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(model.state_dict(), 'notebooks/gpt.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sentence: Great minds think alike, but\n",
      "greedy: Great minds think alike, but they also think differentlyhen clockreatreat minds shine creat minds shine brightly worth beat decre choose integrity they require what weighs\n",
      "top_k_top_p: Great minds think alike, but they also think differentlyhen clockreat minds shines creat minds shine brightly scat minds shineson they some crossreat\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model, start_sentence, max_len):\n",
    "    generated = tokenizer.encode(start_sentence)\n",
    "    input_seq = np.array(generated)\n",
    "    \n",
    "    model.eval()\n",
    "    while len(input_seq[0]) < max_len:\n",
    "        logits = model(input_seq)\n",
    "        next_token_logits = logits[-1, -1]\n",
    "        next_token = np.argmax(next_token_logits).item()\n",
    "        generated[0].append(next_token)\n",
    "        if next_token == tokenizer.EOS:\n",
    "            break\n",
    "        input_seq = np.array(generated)\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)[0]\n",
    "\n",
    "def generate_with_top_k_and_top_p(model, start_sentence, max_len, top_k=0, top_p=1.0):\n",
    "    def top_k_top_p_filtering(logits, top_k=0, top_p=0.1):\n",
    "        filter_value = float('-inf')\n",
    "        if top_k > 0:\n",
    "            indices_to_remove = logits < np.take_along_axis(logits, np.argsort(-logits,axis=-1)[:,:top_k], axis=-1)[..., -1, None]\n",
    "            logits[indices_to_remove] = filter_value\n",
    "        \n",
    "        if top_p < 1.:\n",
    "            sorted_indices = np.argsort(-logits)\n",
    "            sorted_logits = -np.sort(-logits)\n",
    "            \n",
    "            cumulative_probs = np.cumsum(softmax(sorted_logits, dim=-1), axis=-1)\n",
    "            \n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            \n",
    "            # shift\n",
    "            sorted_indices_to_remove = np.roll(sorted_indices_to_remove, 1)\n",
    "            sorted_indices_to_remove[:,0] = 0\n",
    "            \n",
    "            indices_to_remove = np.zeros_like(logits, dtype=bool)\n",
    "            np.put_along_axis(indices_to_remove, sorted_indices, sorted_indices_to_remove, axis=-1)\n",
    "            \n",
    "            logits[indices_to_remove] = filter_value\n",
    "            \n",
    "        return logits\n",
    "    \n",
    "    generated = tokenizer.encode(start_sentence)\n",
    "    input_seq = np.array(generated)\n",
    "    \n",
    "    model.eval()\n",
    "    while len(input_seq[0]) < max_len:\n",
    "        logits = model(input_seq)\n",
    "        next_token_logits = logits[-1]\n",
    "        next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "        probs = softmax(next_token_logits, dim=-1)[-1]\n",
    "        next_token = np.random.choice(np.arange(probs.shape[0]), size=1, p=probs)\n",
    "        generated[0].append(next_token[0])\n",
    "        if next_token == tokenizer.EOS:\n",
    "            break\n",
    "        input_seq = np.array(generated)\n",
    "        \n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)[0]\n",
    "\n",
    "# Great minds think alike, but they also think differently.\n",
    "input_sentence = \"Great minds think alike, but\"\n",
    "print(f'input_sentence: {input_sentence}')\n",
    "print(f'greedy: {generate_sentence(model, input_sentence, max_len=max_len)}')\n",
    "print(f'top_k_top_p: {generate_with_top_k_and_top_p(model, input_sentence, max_len=max_len, top_k=64, top_p=0.8)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcrot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
