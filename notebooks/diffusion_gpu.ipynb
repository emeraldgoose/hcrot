{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "142da9b1",
            "metadata": {},
            "source": [
                "# GPU Accelerated Diffusion Model with hcrot\n",
                "\n",
                "This notebook demonstrates training and inference of a DDPM (Denoising Diffusion Probabilistic Model) using the `hcrot` library with **CuPy GPU acceleration**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "de943564",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import warnings\n",
                "warnings.filterwarnings(action='ignore')\n",
                "\n",
                "# Ensure we are in the root directory to access datasets\n",
                "if os.path.basename(os.getcwd()) == 'notebooks':\n",
                "    os.chdir('..')\n",
                "\n",
                "from typing import *\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm.auto import tqdm, trange\n",
                "\n",
                "import hcrot\n",
                "from hcrot import layers, optim"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b31323c7",
            "metadata": {},
            "outputs": [],
            "source": [
                "class Model(layers.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.unet = layers.UNetModel(\n",
                "            sample_size=14,\n",
                "            in_channels=1,\n",
                "            out_channels=1,\n",
                "            block_out_channels=(32, 64, 32),\n",
                "            num_class_embeds=10,\n",
                "        )\n",
                "\n",
                "    def forward(self, x_noisy, t, labels):\n",
                "        return self.unet(x_noisy, t, labels)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "82ccba18",
            "metadata": {},
            "outputs": [],
            "source": [
                "def average_pooling(img, pool_size=2):\n",
                "    B, C, H, W = img.shape\n",
                "    new_H, new_W = H // pool_size, W // pool_size\n",
                "    img = img[:, :, :new_H * pool_size, :new_W * pool_size]\n",
                "    img_reshaped = img.reshape(B, C, new_H, pool_size, new_W, pool_size)\n",
                "    downsampled = img_reshaped.mean(axis=(3, 5))\n",
                "    return downsampled\n",
                "\n",
                "batch_size = 512\n",
                "num_epochs = 10\n",
                "timesteps = 1000\n",
                "lr = 1e-4\n",
                "\n",
                "device = 'cuda' # Set to 'cuda' for GPU acceleration\n",
                "\n",
                "print(\"Loading and preprocessing data...\")\n",
                "df = pd.read_csv('./datasets/mnist_test.csv')\n",
                "label = df['7'].to_numpy()\n",
                "df = df.drop('7', axis=1)\n",
                "dat = df.to_numpy()\n",
                "\n",
                "mnist = dat[:batch_size * 10]\n",
                "train_label = label[:batch_size * 10]\n",
                "mnist = mnist.reshape(-1, 1, 28, 28).astype(np.float32)\n",
                "mnist = (mnist / 255.) * 2. - 1.\n",
                "mnist = average_pooling(mnist, 2) # resize to 14x14\n",
                "\n",
                "print(f\"Moving model and dataloader to {device}...\")\n",
                "dataloader = hcrot.dataset.Dataloader(mnist, train_label, batch_size=batch_size, shuffle=True).to(device)\n",
                "model = Model().to(device)\n",
                "\n",
                "optimizer = hcrot.optim.AdamW(model, lr_rate=lr)\n",
                "criterion = layers.MSELoss()\n",
                "noise_scheduler = layers.DDPMScheduler(num_train_timesteps=timesteps, beta_schedule='squaredcos_cap_v2')\n",
                "\n",
                "print(\"Starting GPU training...\")\n",
                "pbar = trange(num_epochs)\n",
                "for epoch in pbar:\n",
                "    total_loss = 0\n",
                "    for i, (x, labels) in enumerate(dataloader):\n",
                "        t = np.random.randint(0, timesteps, (x.shape[0],))\n",
                "        noise = np.random.randn(*x.shape)\n",
                "        noisy_x = noise_scheduler.add_noise(x, noise, t)\n",
                "\n",
                "        noise_pred = model(noisy_x, t, labels)\n",
                "        loss = criterion(noise_pred, noise)\n",
                "        total_loss += loss.item()\n",
                "        \n",
                "        dz = criterion.backward()\n",
                "        optimizer.update(dz)\n",
                "    \n",
                "    pbar.set_postfix(loss=total_loss/(i+1))\n",
                "\n",
                "print(\"Training finished.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4eaca853",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Inference on GPU\n",
                "print(\"Starting inference...\")\n",
                "model.eval()\n",
                "record_steps = [0, 200, 400, 600, 800, 999]\n",
                "latents = np.random.randn(1, 1, 14, 14)\n",
                "\n",
                "if device == 'cuda':\n",
                "    import cupy as cp\n",
                "    latents = cp.asarray(latents)\n",
                "\n",
                "target_label = np.array([5]) # Generate digit '5'\n",
                "\n",
                "noise_scheduler.set_timesteps(num_inference_steps=1000)\n",
                "reverse_process = []\n",
                "\n",
                "for t in tqdm(noise_scheduler.timesteps):\n",
                "    noise_pred = model(latents, t, target_label)\n",
                "    latents = noise_scheduler.step(noise_pred, t, latents).prev_sample\n",
                "    \n",
                "    if t in record_steps:\n",
                "        img = latents[0, 0]\n",
                "        if hasattr(img, 'get'): img = img.get()\n",
                "        reverse_process.append(img)\n",
                "\n",
                "fig, ax = plt.subplots(1, len(reverse_process), figsize=(15, 3))\n",
                "for i, img in enumerate(reverse_process):\n",
                "    ax[i].imshow(img, cmap='gray')\n",
                "    ax[i].axis('off')\n",
                "    ax[i].set_title(f\"T={record_steps[i]}\")\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "hcrot",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
