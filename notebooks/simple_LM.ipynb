{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from hcrot import layers, optim\n",
    "from hcrot.dataset import *\n",
    "from hcrot.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The sun always rises after the darkest night, so keep hope alive\",\n",
    "    \"A bird in the hand is worth two in the bush, so hold on tightly\",\n",
    "    \"Fortune favors the brave, but wisdom guides their steps\",\n",
    "    \"The squeaky wheel gets the grease, so speak up when needed\",\n",
    "    \"A watched pot never boils, so do not waste your time staring\",\n",
    "    \"He who hesitates is lost, so act decisively when the time comes\",\n",
    "    \"A penny saved is a penny earned, so start saving early\",\n",
    "    \"A chain is only as strong as its weakest link, so strengthen every part\",\n",
    "    \"Absence makes the heart grow fonder, but do not forget to stay in touch\",\n",
    "    \"Do unto others as you would have them do unto you, and live with kindness\",\n",
    "    \"The harder you work, the luckier you get, so never stop trying\",\n",
    "    \"Patience is a virtue, but persistence brings success\",\n",
    "    \"A rolling stone gathers no moss, so keep moving forward\",\n",
    "    \"Great minds think alike, but they also think differently\",\n",
    "    \"Birds of a feather flock together, so choose your company wisely\",\n",
    "    \"A fool and his money are soon parted, so spend with caution\",\n",
    "    \"Time heals all wounds, but some scars may remain\",\n",
    "    \"A stitch in time saves nine, so do not delay your efforts\",\n",
    "    \"Honesty is the best policy, but tact is also important\",\n",
    "    \"If it is not broken, do not fix it, but always seek improvement\",\n",
    "    \"Necessity is the mother of invention, so embrace challenges\",\n",
    "    \"Do not bite the hand that feeds you, but show gratitude instead\",\n",
    "    \"An ounce of prevention is worth a pound of cure, so plan ahead\",\n",
    "    \"A little knowledge is a dangerous thing, so always keep learning\",\n",
    "    \"Practice what you preach, but also be open to feedback\",\n",
    "    \"Where there is smoke, there is fire, so investigate further\",\n",
    "    \"Every dog has its day, so your time will come too\",\n",
    "    \"Do not put all your eggs in one basket, but diversify your risks\",\n",
    "    \"The best things in life are free, but they require effort\",\n",
    "    \"A journey well planned is a journey half completed\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splited_words = set()\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split():\n",
    "        splited_words.add(word)\n",
    "\n",
    "# shuffle\n",
    "vocab = {token: idx for idx, token in enumerate(splited_words)}\n",
    "vocab['<pad>'] = len(vocab)\n",
    "vocab['<sos>'] = len(vocab)\n",
    "vocab['<eos>'] = len(vocab)\n",
    "vocab['[MASK]'] = len(vocab)\n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return [vocab[word] for word in sentence.split()]\n",
    "\n",
    "data = [tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "max_len = max(len(sentence) for sentence in data)\n",
    "padded_data = [[vocab['<sos>']] + sentence + [vocab['<eos>']] + [vocab['<pad>']] * (max_len - len(sentence)) for sentence in data]\n",
    "padded_data = np.array(padded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
    "    # refs: https://paul-hyun.github.io/transformer-01/\n",
    "    def cal_angle(position, i_hidn):\n",
    "        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\n",
    "\n",
    "    return sinusoid_table\n",
    "\n",
    "class GPT(layers.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, max_len=16):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding = layers.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = np.expand_dims(get_sinusoid_encoding_table(max_len, embed_size), axis=0)\n",
    "        self.transformer_decoder_layer = layers.TransformerDecoderLayer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_size * 4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = layers.TransformerDecoder(\n",
    "            self.transformer_decoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.fc_out = layers.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, tgt):\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_mask = self._generate_square_subsequent_mask(tgt_len)\n",
    "\n",
    "        tgt_emb = self.embedding(tgt) + self.positional_encoding[:, :tgt_len, :]\n",
    "\n",
    "        output = self.transformer_decoder(tgt_emb, tgt_emb, tgt_mask=tgt_mask)\n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = np.triu(np.ones((sz, sz)), 1)\n",
    "        return mask\n",
    "\n",
    "class BERT(layers.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, max_len=17):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding = layers.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = np.expand_dims(get_sinusoid_encoding_table(max_len, embed_size), axis=0)\n",
    "        self.transformer_encoder_layer = layers.TransformerEncoderLayer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_size * 4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = layers.TransformerEncoder(\n",
    "            self.transformer_encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc_out = layers.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src_len = src.shape[1]\n",
    "        src_emb = self.embedding(src) + self.positional_encoding[:, :src_len, :]\n",
    "        \n",
    "        output = self.transformer_encoder(src_emb)\n",
    "        output = self.fc_out(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.040975: 100%|██████████| 100/100 [01:43<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "num_heads = 8\n",
    "num_layers = 2\n",
    "\n",
    "gpt = GPT(vocab_size, embed_size, num_heads, num_layers)\n",
    "criterion = layers.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gpt, lr_rate=1e-3)\n",
    "\n",
    "inputs = padded_data[:, :-1]\n",
    "targets = padded_data[:, 1:]\n",
    "bsz, seq_len = inputs.shape\n",
    "\n",
    "num_epochs = 100\n",
    "pbar = tqdm(range(num_epochs))\n",
    "for epoch in pbar:\n",
    "    outputs = gpt(inputs)\n",
    "    \n",
    "    outputs = outputs.reshape(-1, vocab_size)\n",
    "    targets = targets.reshape(-1)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    dz = criterion.backward()\n",
    "    dz = dz.reshape(bsz, seq_len, -1)\n",
    "    optimizer.update(dz)\n",
    "    \n",
    "    pbar.set_description(f'Loss: {loss.item():5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.487052: 100%|██████████| 100/100 [01:03<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "embed_size = 128\n",
    "num_heads = 8\n",
    "num_layers = 2\n",
    "\n",
    "bert = BERT(vocab_size, embed_size, num_heads, num_layers)\n",
    "criterion = layers.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(bert, lr_rate=1e-3)\n",
    "\n",
    "targets = padded_data\n",
    "bsz, seq_len = targets.shape\n",
    "\n",
    "num_epochs = 100\n",
    "pbar = tqdm(range(num_epochs))\n",
    "for epoch in pbar:\n",
    "    inputs = copy.deepcopy(padded_data)\n",
    "\n",
    "    # random masking\n",
    "    # masking ratio: 30%\n",
    "    for i in range(len(inputs)):\n",
    "        sentence = inputs[i]\n",
    "        masking_ids = np.random.randint(1,len(sentence)-1,size=int(len(sentence)*0.3))\n",
    "        for idx in masking_ids:\n",
    "            inputs[i][idx] = vocab['[MASK]']\n",
    "    \n",
    "    outputs = bert(inputs)\n",
    "    \n",
    "    outputs = outputs.reshape(-1, vocab_size)\n",
    "    targets = targets.reshape(-1)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    dz = criterion.backward()\n",
    "    dz = dz.reshape(bsz, seq_len, -1)\n",
    "    optimizer.update(dz)\n",
    "    \n",
    "    pbar.set_description(f'Loss: {loss.item():5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: Great minds think alike, but they also think differently\n",
      "input_sentence: Great minds think alike, but\n",
      "greedy: Great minds think alike, but they also be open to feedback Great minds think alike, but\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model, start_sentence, max_len):\n",
    "    generated = [vocab[token] for token in start_sentence.split()]\n",
    "    input_seq = np.expand_dims(np.array(generated),0)\n",
    "    \n",
    "    while len(input_seq[0]) < max_len:\n",
    "        # print(' '.join(inverse_vocab[token] for token in input_seq[0]))\n",
    "        output = model.forward(input_seq)\n",
    "        next_token_logits = output[-1, -1]\n",
    "        next_token = np.argmax(next_token_logits).item()\n",
    "        generated.append(next_token)\n",
    "        if next_token == vocab['<eos>']:\n",
    "            break\n",
    "        input_seq = np.array([generated])\n",
    "        \n",
    "    return ' '.join(inverse_vocab[token] for token in generated)\n",
    "\n",
    "sentence = \"Great minds think alike, but they also think differently\"\n",
    "input_sentence = \"Great minds think alike, but\"\n",
    "print(f\"original: {sentence}\")\n",
    "print(f'input_sentence: {input_sentence}')\n",
    "print(f'greedy: {generate_sentence(gpt, input_sentence, max_len=16)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: Great minds think alike, but they also think differently\n",
      "predict: <sos> Great minds think alike, but they also think differently <eos>\n",
      "token(prob): think(0.34) | alike,(0.18) | they(0.05) | is(0.02) | Honesty(0.01)\n"
     ]
    }
   ],
   "source": [
    "from hcrot.utils import softmax\n",
    "\n",
    "def bert_predict_mask_token(model, sentence):\n",
    "    inputs = tokenize(sentence)\n",
    "    inputs = [[vocab['<sos>']] + inputs + [vocab['<eos>']]]\n",
    "    mask_idx = inputs[0].index(vocab['[MASK]'])\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    output = model.forward(inputs)\n",
    "\n",
    "    logits = softmax(output[0][mask_idx])\n",
    "    predict = sorted([(inverse_vocab[token],round(logit,2)) for token, logit in enumerate(logits)],key=lambda e: -e[1])[:5]\n",
    "    \n",
    "    output = output.argmax(-1)[0]\n",
    "    return ' '.join(inverse_vocab[token] for token in output), predict\n",
    "\n",
    "sentence = \"Great minds think alike, but they also think differently\"\n",
    "masked_sentence = \"Great minds [MASK] alike, but they also think differently\"\n",
    "prediction, token_logits = bert_predict_mask_token(bert, masked_sentence)\n",
    "print(f\"original: {sentence}\")\n",
    "print(f\"predict: {prediction}\")\n",
    "print(f\"token(prob): {' | '.join([f'{token}({logit})' for token, logit in token_logits])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcrot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
