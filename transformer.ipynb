{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from hcrot import layers, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"A journey of a thousand miles begins with a single step\",\n",
    "    \"To be or not to be that is the question\",\n",
    "    \"All that glitters is not gold but it is very valuable\",\n",
    "    \"Knowledge is power but enthusiasm pulls the switch\",\n",
    "    \"The only thing we have to fear is fear itself\",\n",
    "    \"In the end we will remember not the words of our enemies\",\n",
    "    \"Life is what happens when you’re busy making other plans\",\n",
    "    \"To succeed in life you need two things ignorance and confidence\",\n",
    "    \"The future belongs to those who believe in the beauty of their dreams\"\n",
    "]\n",
    "\n",
    "vocab = {}\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "vocab['<pad>'] = len(vocab.keys())\n",
    "vocab['<eos>'] = len(vocab.keys())\n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return [vocab[word] for word in sentence.split()]\n",
    "\n",
    "data = [tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "max_len = max(len(sentence) for sentence in data)\n",
    "padded_data = [sentence + [vocab['<pad>']] * (max_len - len(sentence)) + [vocab['<eos>']] for sentence in data]\n",
    "padded_data = np.array(padded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
    "    # refs: https://paul-hyun.github.io/transformer-01/\n",
    "    def cal_angle(position, i_hidn):\n",
    "        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\n",
    "\n",
    "    return sinusoid_table\n",
    "\n",
    "class GPT(layers.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, max_len=512):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding = layers.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = np.expand_dims(get_sinusoid_encoding_table(max_len, embed_size), axis=0)\n",
    "        self.transformer_decoder_layer = layers.TransformerDecoderLayer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_size * 4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = layers.TransformerDecoder(\n",
    "            self.transformer_decoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.fc_out = layers.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, tgt):\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_mask = self._generate_square_subsequent_mask(tgt_len)\n",
    "\n",
    "        tgt_emb = self.embedding(tgt) + self.positional_encoding[:, :tgt.shape[1], :]\n",
    "\n",
    "        output = self.transformer_decoder(tgt_emb, tgt_emb, tgt_mask=tgt_mask)\n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = np.triu(np.ones((sz, sz)), 1)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 4.470611729039826\n",
      "Epoch 2/50, Loss: 3.822672433568201\n",
      "Epoch 3/50, Loss: 3.305046282255573\n",
      "Epoch 4/50, Loss: 2.948956121846243\n",
      "Epoch 5/50, Loss: 2.347445292140538\n",
      "Epoch 6/50, Loss: 1.891882693010152\n",
      "Epoch 7/50, Loss: 1.541612920097274\n",
      "Epoch 8/50, Loss: 1.1817331319095838\n",
      "Epoch 9/50, Loss: 0.9197488995104414\n",
      "Epoch 10/50, Loss: 0.684036989145343\n",
      "Epoch 11/50, Loss: 0.5085725404061618\n",
      "Epoch 12/50, Loss: 0.36315022370106964\n",
      "Epoch 13/50, Loss: 0.2658197968044208\n",
      "Epoch 14/50, Loss: 0.2063143595293862\n",
      "Epoch 15/50, Loss: 0.1597128879574305\n",
      "Epoch 16/50, Loss: 0.12694673902448858\n",
      "Epoch 17/50, Loss: 0.1050292605532066\n",
      "Epoch 18/50, Loss: 0.08627863200116671\n",
      "Epoch 19/50, Loss: 0.07179282088260787\n",
      "Epoch 20/50, Loss: 0.06232451579747673\n",
      "Epoch 21/50, Loss: 0.05056625525856857\n",
      "Epoch 22/50, Loss: 0.045285526152731036\n",
      "Epoch 23/50, Loss: 0.039436409540196414\n",
      "Epoch 24/50, Loss: 0.03550112554714281\n",
      "Epoch 25/50, Loss: 0.03308612833875222\n",
      "Epoch 26/50, Loss: 0.027807124843697965\n",
      "Epoch 27/50, Loss: 0.025720979692201138\n",
      "Epoch 28/50, Loss: 0.022611393746742533\n",
      "Epoch 29/50, Loss: 0.021377756524294043\n",
      "Epoch 30/50, Loss: 0.019693409063541077\n",
      "Epoch 31/50, Loss: 0.018299482558833926\n",
      "Epoch 32/50, Loss: 0.017197186554620426\n",
      "Epoch 33/50, Loss: 0.015575475386701546\n",
      "Epoch 34/50, Loss: 0.014853955692958254\n",
      "Epoch 35/50, Loss: 0.013844265774331822\n",
      "Epoch 36/50, Loss: 0.012982764842608331\n",
      "Epoch 37/50, Loss: 0.012086884429606424\n",
      "Epoch 38/50, Loss: 0.011706405946132118\n",
      "Epoch 39/50, Loss: 0.011873555944327827\n",
      "Epoch 40/50, Loss: 0.010699759684069098\n",
      "Epoch 41/50, Loss: 0.010317816036535933\n",
      "Epoch 42/50, Loss: 0.009413750837545087\n",
      "Epoch 43/50, Loss: 0.00925931485696737\n",
      "Epoch 44/50, Loss: 0.008947729134931257\n",
      "Epoch 45/50, Loss: 0.008746862702687822\n",
      "Epoch 46/50, Loss: 0.008229439627105156\n",
      "Epoch 47/50, Loss: 0.00799639962914081\n",
      "Epoch 48/50, Loss: 0.007863913700341507\n",
      "Epoch 49/50, Loss: 0.007467522573802974\n",
      "Epoch 50/50, Loss: 0.007600108173564774\n"
     ]
    }
   ],
   "source": [
    "embed_size = 256\n",
    "num_heads = 4\n",
    "num_layers = 3\n",
    "\n",
    "model = GPT(vocab_size, embed_size, num_heads, num_layers)\n",
    "criterion = layers.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model, lr_rate=1e-3)\n",
    "\n",
    "inputs = padded_data[:, :-1]\n",
    "targets = padded_data[:, 1:]\n",
    "bsz, seq_len = inputs.shape\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model.forward(inputs)\n",
    "    \n",
    "    outputs = outputs.reshape(-1, vocab_size)\n",
    "    targets = targets.reshape(-1)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    dz = criterion.backward()\n",
    "    dz = dz.reshape(bsz, seq_len, -1)\n",
    "    optimizer.update(dz)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox jumps over the lazy dog',\n",
       " 'A journey of a thousand miles begins with a single step',\n",
       " 'To be or not to be that is the question',\n",
       " 'All that glitters is not gold but it is very valuable',\n",
       " 'Knowledge is power but enthusiasm pulls the switch',\n",
       " 'The only thing we have to fear is fear itself',\n",
       " 'In the end we will remember not the words of our enemies',\n",
       " 'Life is what happens when you’re busy making other plans',\n",
       " 'To succeed in life you need two things ignorance and confidence',\n",
       " 'The future belongs to those who believe in the beauty of their dreams']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future belongs to those who believe in the beauty of their <eos>\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model, start_sentence, max_len):\n",
    "    generated = [vocab[token] for token in start_sentence.split()]\n",
    "    input_seq = np.expand_dims(np.array(generated),0)\n",
    "    \n",
    "    while len(input_seq) < max_len:\n",
    "        output = model.forward(input_seq)\n",
    "        next_token_logits = output[-1, -1]\n",
    "        next_token = np.argmax(next_token_logits).item()\n",
    "        generated.append(next_token)\n",
    "        if next_token == vocab['<eos>']:\n",
    "            break\n",
    "        input_seq = np.array([generated])\n",
    "        \n",
    "    return ' '.join(inverse_vocab[token] for token in generated)\n",
    "\n",
    "print(generate_sentence(model, 'The', max_len=100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
